{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34019808-4fc9-418b-95d4-e10867d66df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama running: True\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "def check_if_running(process_name):\n",
    "    running = False\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "    return running\n",
    "\n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\"Ollama not running. Launch ollama before proceeding.\")\n",
    "print(\"Ollama running:\", check_if_running(\"ollama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f96047f-86c4-4cc0-bfe7-c5c044d3e526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_path = \"instruction-data-with-response.json\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    test_data = json.load(file)\n",
    "\n",
    "\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f91a9d5-2e42-493b-a06a-dfcc894b248c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 Egyptian rappers:\n",
      "\n",
      "1. Tamer Hosny - Known for his unique blend of Arabic and English lyrics.\n",
      "2. Mohamed Ramadan - A popular rapper and singer who has collaborated with international artists.\n",
      "3. Ahmed Adel - A rising star in the Egyptian hip-hop scene, known for his energetic live performances.\n",
      "4. Karim Hafez - A veteran rapper and songwriter who has been active in the industry since the 1990s.\n",
      "5. Mohamed Mounir - A rapper and singer who has gained a large following in Egypt and the Middle East with his catchy hooks and lyrics.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "def query_model(\n",
    "    prompt,\n",
    "    model=\"llama3.2\",\n",
    "    url=\"http://localhost:11434/api/chat\"\n",
    "):\n",
    "    # Create the data payload as a dictionary\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"options\": {     # Settings below are required for deterministic responses\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 2048\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    # Convert the dictionary to a JSON formatted string and encode it to bytes\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "\n",
    "    # Create a request object, setting the method to POST and adding necessary headers\n",
    "    request = urllib.request.Request(\n",
    "        url,\n",
    "        data=payload,\n",
    "        method=\"POST\"\n",
    "    )\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    # Send the request and capture the response\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        # Read and decode the response\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "\n",
    "    return response_data\n",
    "\n",
    "\n",
    "model = \"llama3.2\"\n",
    "result = query_model(\"list 5 egyptian rappers, think step by step\", model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f2bdfce-2cba-4337-b6d5-b32fc8190480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a cheetah.\n",
      "\n",
      "Score:\n",
      ">> To rewrite the sentence using a simile, we need to compare the speed of the car to something else.\n",
      "\n",
      "Correct output: The car is as fast as lightning.\n",
      "\n",
      "Score: 100\n",
      "\n",
      "The model response \"The car is as fast as a cheetah.\" is not correct because it uses a metaphor (a direct comparison) instead of a simile (a comparison using \"like\" or \"as\"). A simile should use the words \"like\" or \"as\" to make the comparison, which is what the original instruction asked for.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud associated with thunderstorms is a cumulus cloud.\n",
      "\n",
      "Score:\n",
      ">> I would rate the model response a 20.\n",
      "\n",
      "The reason for this low score is that the model response contains an error in its classification of clouds. Cumulonimbus clouds are indeed associated with thunderstorms, but cumulus clouds are typically associated with fair weather and are often seen on warm, sunny days. The correct term should be \"cumulonimbus\" instead of \"cumulus\".\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "\n",
      "Score:\n",
      ">> ### Input\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "### Output\n",
      "Jane Austen.\n",
      "\n",
      "### Score: 100/100\n",
      "\n",
      "The response is complete, accurate, and concise, making it a perfect score. The sentence structure is correct, and the information provided is relevant to the question asked.\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for entry in test_data[:3]:\n",
    "    prompt = (\n",
    "        f\"Given the input `{format_input(entry)}` \"\n",
    "        f\"and correct output `{entry['output']}`, \"\n",
    "        f\"score the model response `{entry['model_response']}`\"\n",
    "        f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "    )\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    print(\"\\nModel response:\")\n",
    "    print(\">>\", entry[\"model_response\"])\n",
    "    print(\"\\nScore:\")\n",
    "    print(\">>\", query_model(prompt))\n",
    "    print(\"\\n-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68bd42f2-c2e5-443e-93aa-e43de2a2f089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:   1%|▋                                                                        | 1/110 [00:04<08:20,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: 60\n",
      "\n",
      "Explanation: The original sentence \"The car is very fast\" can be rewritten using a simile as \"The car is as fast as lightning.\" This response accurately conveys the idea of speed and uses a common simile to do so.\n",
      "\n",
      "However, the provided correct output \"The car is as fast as a cheetah\" is not entirely accurate. While it does use a simile, it's more of an analogy than a traditional simile. A traditional simile would compare two things using \"like\" or \"as,\" whereas an analogy compares two things without using those words.\n",
      "\n",
      "Therefore, the score of 60 reflects that while the response is close, it's not entirely accurate in terms of simile usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:   9%|██████▌                                                                 | 10/110 [00:25<04:03,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: 40\n",
      "\n",
      "The model's response contains an error in identifying 14 as composite and incorrectly including 19 in both categories.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  15%|██████████▍                                                             | 16/110 [00:39<04:02,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: 90\n",
      "\n",
      "Explanation: The correct classification is:\n",
      "\n",
      "- Mercury: Liquid\n",
      "- Oxygen: Gas\n",
      "- Wood: Solid\n",
      "\n",
      "The model response \"Solid: Mercury / Liquid: Oxygen / Gas: Wood\" is mostly correct but in a different format. It's still accurate, so the score remains high.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  16%|███████████▊                                                            | 18/110 [00:44<03:46,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: 60\n",
      "\n",
      "A note was left by someone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  29%|████████████████████▉                                                   | 32/110 [01:18<03:27,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: 80\n",
      "\n",
      "A piece of cake is a type of dessert, not an idiomatic expression meaning something is easy. The original sentence \"It's a piece of cake\" is a common cliché phrase used to describe tasks that are effortless or require little effort. A more accurate and creative response would be \"It's very easy.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  35%|████████████████████████▊                                               | 38/110 [01:32<03:07,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: 80\n",
      "\n",
      "Explanation: The model's response is mostly correct, but it could be improved by adding more context or providing alternative passive voice constructions. For example, \"The plants were watered\" could also be rephrased as \"Water was applied to the plants.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  48%|██████████████████████████████████▋                                     | 53/110 [02:08<02:25,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: 80\n",
      "\n",
      "The model's response is mostly correct but contains an error in categorization. The bicycle should be classified as \"Vehicles\" instead of \"Bicyclist\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  67%|████████████████████████████████████████████████▍                       | 74/110 [02:58<01:33,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: 90\n",
      "\n",
      "The model response \"Technical document.\" is accurate because it correctly classifies the input text as a technical document, which is a manual providing instructions for installing software. The only reason it's not a perfect score is that the response could be more explicit and detailed, but overall, it accurately completes the request.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  76%|██████████████████████████████████████████████████████▉                 | 84/110 [03:21<01:02,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: 80\n",
      "\n",
      "He remained very calm.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  99%|██████████████████████████████████████████████████████████████████████▎| 109/110 [04:20<00:02,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: 60\n",
      "\n",
      "She never forgets to call.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|███████████████████████████████████████████████████████████████████████| 110/110 [04:22<00:00,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 100 of 110\n",
      "Average score: 49.24\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_model_scores(json_data, json_key, model=\"llama3.2\"):\n",
    "    scores = []\n",
    "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"score the model response `{entry[json_key]}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the integer number only.\"\n",
    "        )\n",
    "        score = query_model(prompt, model)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score: {score}\")\n",
    "            continue\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "scores = generate_model_scores(test_data, \"model_response\")\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c107e6-527b-4c05-94f4-7ed38b3ccb71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
